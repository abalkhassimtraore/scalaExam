{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *`Nom & Prenom : `*\n",
    "\n",
    "## Abal Khassim TRAORE\n",
    "\n",
    "## Arona GUEYE\n",
    "\n",
    "## Fatou CISSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//creation d un dataframe\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`sh.almond::almond-spark:0.10.9`\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.5`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@672020f8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"BD-FS FIRE\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation du DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/archive/*\"\u001b[39m\r\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 4 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"data/archive/*\"\n",
    "val df = spark\n",
    "        .read\n",
    "        .option(\"header\",\"true\")\n",
    "        .csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: string (nullable = true)\n",
      " |-- Price Each: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres3_0\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m186850L\u001b[39m\r\n",
       "\u001b[36mres3_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"Order ID\"\u001b[39m,\n",
       "  \u001b[32m\"Product\"\u001b[39m,\n",
       "  \u001b[32m\"Quantity Ordered\"\u001b[39m,\n",
       "  \u001b[32m\"Price Each\"\u001b[39m,\n",
       "  \u001b[32m\"Order Date\"\u001b[39m,\n",
       "  \u001b[32m\"Purchase Address\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n",
    "df.columns\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage de la colone \"Order Date\" en timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.to_timestamp\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mdfWithTimestamp\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 4 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.to_timestamp\n",
    "import spark.implicits._\n",
    "\n",
    "\n",
    "val dfWithTimestamp = df.withColumn(\"Order Date\", to_timestamp(col(\"Order Date\"), \"MM/dd/yyyy HH:mm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{to_timestamp, year}\n",
       "\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mdfWithTimestamp\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 4 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{to_timestamp, year}\n",
    "\n",
    "\n",
    "val dfWithTimestamp = df.withColumn(\"Order Date\", to_timestamp(col(\"Order Date\"), \"MM/dd/yyyy HH:mm\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Quelle année a été la meilleure en termes de ventes ? Combien a-t-on gagné cette année-là ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La meilleur annee en terme de vente est 2019 avec un total de 3.379903640992789E7 dollars.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfAnnee\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Annee: int, Total: double]\r\n",
       "\u001b[36mbestAnnee\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m19\u001b[39m\r\n",
       "\u001b[36mtotal\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m3.379903640992789E7\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Chiffre d'affaire par mois trier par ordre descroissant\n",
    "val dfAnnee = dfWithTimestamp\n",
    "  .groupBy(year(col(\"Order Date\")).as(\"Annee\"))\n",
    "  .agg(sum(col(\"Quantity Ordered\") * col(\"Price Each\")).as(\"Total\"))\n",
    "  .orderBy(desc(\"Total\"))\n",
    "\n",
    "  \n",
    "\n",
    "// On prend le premier element de la colone\n",
    "val bestAnnee = dfAnnee.first().getAs[Int](\"Annee\")\n",
    "val total = dfAnnee.first().getAs[Double](\"Total\")\n",
    "\n",
    "println(s\"La meilleur annee en terme de vente est 20$bestAnnee avec un total de $total dollars.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Quel mois a été le meilleur en termes de ventes ? Combien a-t-on gagné ce mois-là ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le mois le plus rentable est le mois numéro 11 avec un chiffre d'affaires total de 3715537.46000095 dollars.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfMois\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Mois: int, Total: double]\r\n",
       "\u001b[36mbestMois\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m11\u001b[39m\r\n",
       "\u001b[36mtotal\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m3715537.46000095\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Chiffre d'affaire par mois trier par ordre descroissant\n",
    "val dfMois = dfWithTimestamp\n",
    "  .groupBy(month(col(\"Order Date\")).alias(\"Mois\"))\n",
    "  .agg(sum(col(\"Quantity Ordered\") * col(\"Price Each\")).as(\"Total\"))\n",
    "  .orderBy(desc(\"Total\"))\n",
    "  \n",
    "// On prend le premier element de la colone\n",
    "val bestMois = dfMois.first().getAs[Int](\"Mois\")\n",
    "val total = dfMois.first().getAs[Double](\"Total\")\n",
    "\n",
    "println(s\"Le mois le plus rentable est le mois numéro $bestMois avec un chiffre d'affaires total de $total dollars.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Quelle ville a enregistré le plus grand nombre de ventes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ville qui a enregistré le plus grand nombre de ventes est  San Francisco avec un total de 44732 ventes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfCity\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 5 more fields]\r\n",
       "\u001b[36mdfVenteByCity\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [City: string, Total: bigint]\r\n",
       "\u001b[36mbestCity\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\" San Francisco\"\u001b[39m\r\n",
       "\u001b[36mtotal\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m44732L\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// On split avec comme separateur \",\" et on prend le deuxieme element du tableau pour avoir la ville \n",
    "val dfCity = df.withColumn(\"City\", split(col(\"Purchase Address\"), \",\")(1))\n",
    "\n",
    "// Nombre de ventes par ville trier par ordre descroissant\n",
    "val dfVenteByCity = dfCity\n",
    "  .groupBy(\"City\")\n",
    "  .agg(count(\"City\").alias(\"Total\"))\n",
    "  .orderBy(desc(\"Total\"))\n",
    "\n",
    "// On prend le premier element de la colone\n",
    "val bestCity = dfVenteByCity.first().getAs[String](\"City\")\n",
    "val total = dfVenteByCity.first().getAs[Long](\"Total\")\n",
    "\n",
    "println(s\"La ville qui a enregistré le plus grand nombre de ventes est $bestCity avec un total de $total ventes.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4:À quelle heure devraient-ils diffuser des publicités pour maximiser les chances que lesclients achètent le produit ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'heure optimale pour diffuser des publicités est 19 heure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mhourDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Hour: int]\r\n",
       "\u001b[36mbestHour\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m19\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val hourDF = dfWithTimestamp.select(hour(col(\"Order Date\")).alias(\"Hour\"))\n",
    "\n",
    "val bestHour = hourDF.groupBy(\"Hour\").count().orderBy(desc(\"count\")).first().getAs[Int](\"Hour\")\n",
    "\n",
    "println(s\"L'heure optimale pour diffuser des publicités est $bestHour heure.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5:Quels produits sont le plus souvent vendus ensemble ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------+-----------+\n",
      "|Prod 1                  |Prod 2                    |Occurrences|\n",
      "+------------------------+--------------------------+-----------+\n",
      "|Lightning Charging Cable|iPhone                    |1011       |\n",
      "|USB-C Charging Cable    |Google Phone              |997        |\n",
      "|iPhone                  |Wired Headphones          |462        |\n",
      "|Wired Headphones        |Google Phone              |422        |\n",
      "|Apple Airpods Headphones|iPhone                    |373        |\n",
      "|Vareebadd Phone         |USB-C Charging Cable      |368        |\n",
      "|Google Phone            |Bose SoundSport Headphones|228        |\n",
      "|USB-C Charging Cable    |Wired Headphones          |203        |\n",
      "|Vareebadd Phone         |Wired Headphones          |149        |\n",
      "|Lightning Charging Cable|Wired Headphones          |129        |\n",
      "|Lightning Charging Cable|AA Batteries (4-pack)     |106        |\n",
      "|USB-C Charging Cable    |Bose SoundSport Headphones|102        |\n",
      "|Lightning Charging Cable|USB-C Charging Cable      |100        |\n",
      "|Apple Airpods Headphones|Wired Headphones          |100        |\n",
      "|AAA Batteries (4-pack)  |USB-C Charging Cable      |95         |\n",
      "|AAA Batteries (4-pack)  |AA Batteries (4-pack)     |87         |\n",
      "|AAA Batteries (4-pack)  |Wired Headphones          |86         |\n",
      "|AA Batteries (4-pack)   |Wired Headphones          |83         |\n",
      "|Vareebadd Phone         |Bose SoundSport Headphones|82         |\n",
      "|Apple Airpods Headphones|AAA Batteries (4-pack)    |81         |\n",
      "+------------------------+--------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Products: array<string>]\r\n",
       "\u001b[36mPairs\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Prod 1: string, Prod 2: string]\r\n",
       "\u001b[36mPairCounts\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Prod 1: string, Prod 2: string ... 1 more field]\r\n",
       "\u001b[36mresultat\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Prod 1: string, Prod 2: string ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val df1 = dfWithTimestamp.groupBy(\"Order ID\")\n",
    "  .agg(collect_set(\"Product\").as(\"Products\"))\n",
    "\n",
    "\n",
    "//les paires de produits pour chaque commande\n",
    "val Pairs = df1.flatMap { row =>\n",
    "  val products = row.getAs[Seq[String]](\"Products\")\n",
    "  products.combinations(2).map(pair => (pair(0), pair(1)))\n",
    "}.toDF(\"Prod 1\", \"Prod 2\")\n",
    "\n",
    "\n",
    "// Compter les occurrences des paires de produits\n",
    "val PairCounts = Pairs.groupBy(\"Prod 1\", \"Prod 2\")\n",
    "  .count()\n",
    "  .withColumn(\"Occurrences\", col(\"count\").cast(\"Int\"))\n",
    "  .drop(\"count\")\n",
    "\n",
    "\n",
    "// Trier\n",
    "val resultat = PairCounts.orderBy(col(\"Occurrences\").desc)\n",
    "\n",
    "// Affichez les 10 premiers\n",
    "resultat.show(truncate=false)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6:Quel produit s'est le plus vendu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le produit le plus vendu est : AAA Batteries (4-pack)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mVente\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Product: string, Total: double]\r\n",
       "\u001b[36mtopProduit\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"AAA Batteries (4-pack)\"\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calcul du total des quantités vendues par produit\n",
    "val Vente = dfWithTimestamp.groupBy(\"Product\")\n",
    "  .agg(sum(\"Quantity Ordered\").as(\"Total\"))\n",
    "  .orderBy(col(\"Total\").desc)\n",
    "\n",
    "// Récupération du produit le plus vendu (première ligne)\n",
    "val topProduit = Vente.first().getAs[String](\"Product\")\n",
    "\n",
    "// Affichage du produit le plus vendu\n",
    "println(s\"Le produit le plus vendu est : $topProduit\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les piles AAA sont des articles couramment utilisés dans de nombreux appareils électroniques en temoigne la reponse a la question 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+-------------------+--------------------+-------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|         Order Date|    Purchase Address|USB-C_Ordered|\n",
      "+--------+--------------------+----------------+----------+-------------------+--------------------+-------------+\n",
      "|  295665|  Macbook Pro Laptop|               1|      1700|0019-12-30 00:01:00|136 Church St, Ne...|            0|\n",
      "|  295666|  LG Washing Machine|               1|     600.0|0019-12-29 07:03:00|562 2nd St, New Y...|            0|\n",
      "|  295667|USB-C Charging Cable|               1|     11.95|0019-12-12 18:21:00|277 Main St, New ...|            1|\n",
      "|  295668|    27in FHD Monitor|               1|    149.99|0019-12-22 15:13:00|410 6th St, San F...|            0|\n",
      "|  295669|USB-C Charging Cable|               1|     11.95|0019-12-18 12:38:00|43 Hill St, Atlan...|            1|\n",
      "|  295670|AA Batteries (4-p...|               1|      3.84|0019-12-31 22:58:00|200 Jefferson St,...|            0|\n",
      "|  295671|USB-C Charging Cable|               1|     11.95|0019-12-16 15:10:00|928 12th St, Port...|            1|\n",
      "|  295672|USB-C Charging Cable|               2|     11.95|0019-12-13 09:29:00|813 Hickory St, D...|            1|\n",
      "|  295673|Bose SoundSport H...|               1|     99.99|0019-12-15 23:26:00|718 Wilson St, Da...|            0|\n",
      "|  295674|AAA Batteries (4-...|               4|      2.99|0019-12-28 11:51:00|77 7th St, Dallas...|            0|\n",
      "|  295675|USB-C Charging Cable|               2|     11.95|0019-12-13 13:52:00|594 1st St, San F...|            1|\n",
      "|  295676|     ThinkPad Laptop|               1|    999.99|0019-12-28 17:19:00|410 Lincoln St, L...|            0|\n",
      "|  295677|AA Batteries (4-p...|               2|      3.84|0019-12-20 19:19:00|866 Pine St, Bost...|            0|\n",
      "|  295678|AAA Batteries (4-...|               2|      2.99|0019-12-06 09:38:00|187 Lincoln St, D...|            0|\n",
      "|  295679|USB-C Charging Cable|               1|     11.95|0019-12-25 09:39:00|902 2nd St, Dalla...|            1|\n",
      "|  295680|Lightning Chargin...|               1|     14.95|0019-12-01 14:30:00|338 Main St, Aust...|            0|\n",
      "|  295681|        Google Phone|               1|       600|0019-12-25 12:37:00|79 Elm St, Boston...|            0|\n",
      "|  295681|USB-C Charging Cable|               1|     11.95|0019-12-25 12:37:00|79 Elm St, Boston...|            1|\n",
      "|  295681|Bose SoundSport H...|               1|     99.99|0019-12-25 12:37:00|79 Elm St, Boston...|            0|\n",
      "|  295681|    Wired Headphones|               1|     11.99|0019-12-25 12:37:00|79 Elm St, Boston...|            0|\n",
      "+--------+--------------------+----------------+----------+-------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfWithLabel\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 5 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithLabel = dfWithTimestamp.withColumn(\"USB-C_Ordered\", when(col(\"Product\") === \"USB-C Charging Cable\", 1).otherwise(0))\n",
    "dfWithLabel.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation et entrainement d'un  modèle de classification de régression logistique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/25 21:35:24 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/05/25 21:35:24 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/05/25 21:35:24 INFO LBFGS: Step Size: 16,36\n",
      "23/05/25 21:35:24 INFO LBFGS: Val and Grad Norm: 0,344724 (rel: 0,0444) 0,134825\n",
      "23/05/25 21:35:24 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:24 INFO LBFGS: Val and Grad Norm: 0,322710 (rel: 0,0639) 0,0574524\n",
      "23/05/25 21:35:24 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:24 INFO LBFGS: Val and Grad Norm: 0,319214 (rel: 0,0108) 0,0188191\n",
      "23/05/25 21:35:24 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:24 INFO LBFGS: Val and Grad Norm: 0,312501 (rel: 0,0210) 0,0184545\n",
      "23/05/25 21:35:24 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:24 INFO LBFGS: Val and Grad Norm: 0,304033 (rel: 0,0271) 0,0140235\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,296924 (rel: 0,0234) 0,0487951\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,294327 (rel: 0,00875) 0,0111071\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,293491 (rel: 0,00284) 0,000884074\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292967 (rel: 0,00178) 0,00357514\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292858 (rel: 0,000372) 0,00157381\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292850 (rel: 2,71e-05) 0,000565384\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292850 (rel: 2,02e-06) 0,000249520\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292850 (rel: 4,65e-07) 5,68451e-05\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292850 (rel: 3,25e-08) 1,71261e-05\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292850 (rel: 1,04e-09) 7,30729e-07\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292850 (rel: 1,01e-10) 2,99895e-07\n",
      "23/05/25 21:35:25 INFO LBFGS: Step Size: 1,000\n",
      "23/05/25 21:35:25 INFO LBFGS: Val and Grad Norm: 0,292850 (rel: 2,65e-12) 1,28047e-08\n",
      "23/05/25 21:35:25 INFO LBFGS: Converged because gradient converged\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.VectorAssembler\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.LogisticRegression\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.Pipeline\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{IntegerType, DoubleType}\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mnumericDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 4 more fields]\r\n",
       "\u001b[36mlabeledDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 5 more fields]\r\n",
       "\u001b[36mtrainingData\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Order ID: string, Product: string ... 5 more fields]\r\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Order ID: string, Product: string ... 5 more fields]\r\n",
       "\u001b[36massembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_27e11b4d3be7\r\n",
       "\u001b[36mlogisticRegression\u001b[39m: \u001b[32mLogisticRegression\u001b[39m = logreg_e283c03cce54\r\n",
       "\u001b[36mpipeline\u001b[39m: \u001b[32mPipeline\u001b[39m = pipeline_24ae18bfc571\r\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mPipelineModel\u001b[39m = pipeline_24ae18bfc571\r\n",
       "\u001b[36mpredictions\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 9 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, DoubleType}\n",
    "\n",
    "val numericDF = df.withColumn(\"Quantity Ordered\", col(\"Quantity Ordered\").cast(IntegerType))\n",
    "  .withColumn(\"Price Each\", col(\"Price Each\").cast(DoubleType)).na.drop()\n",
    "\n",
    "\n",
    "\n",
    "val labeledDF = numericDF.withColumn(\"label\", when(col(\"Product\") === \"USB-C Charging Cable\", 1).otherwise(0))\n",
    "\n",
    "val Array(trainingData, testData) = labeledDF.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"Quantity Ordered\", \"Price Each\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val logisticRegression = new LogisticRegression()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(assembler, logisticRegression))\n",
    "\n",
    "val model = pipeline.fit(trainingData)\n",
    "\n",
    "\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7:Quelle est la probabilité que les prochains clients commandent un câble de chargement USB-C ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité de classe positive pour le prochain client qui achète un câble de chargement USB-C : 0,21345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnextClientProduct\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"USB-C Charging Cable\"\u001b[39m\r\n",
       "\u001b[36mnextPredictions\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 8 more fields]\r\n",
       "\u001b[36mprobabilityVector\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mDenseVector\u001b[39m = [0.7865468387155611,0.21345316128443895]\r\n",
       "\u001b[36mpositiveClass\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.21345316128443895\u001b[39m\r\n",
       "\u001b[36mpositiveClassFormatted\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"0,21345\"\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val nextClientProduct = \"USB-C Charging Cable\"\n",
    "\n",
    "val nextPredictions = model.transform(numericDF.filter(col(\"Product\") === nextClientProduct))\n",
    "val probabilityVector = nextPredictions.select(\"probability\").head().getAs[org.apache.spark.ml.linalg.DenseVector](0)\n",
    "val positiveClass = probabilityVector(1)\n",
    "val positiveClassFormatted = f\"$positiveClass%.5f\"\n",
    "\n",
    "println(s\"Probabilité de classe positive pour le prochain client qui achète un câble de chargement USB-C : $positiveClassFormatted\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8:Quelle est la probabilité que les prochains clients commandent un iPhone ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité de classe positive pour le prochain client qui achète un Iphone : 0,0000000035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnextClientProduct\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"iPhone\"\u001b[39m\r\n",
       "\u001b[36mnextPredictions\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 8 more fields]\r\n",
       "\u001b[36mprobabilityVector\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mDenseVector\u001b[39m = [0.9999999964681876,3.5318122750305667E-9]\r\n",
       "\u001b[36mpositiveClass\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m3.5318122750305667E-9\u001b[39m\r\n",
       "\u001b[36mpositiveClassFormatted\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"0,0000000035\"\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nextClientProduct = \"iPhone\"\n",
    "\n",
    "val nextPredictions = model.transform(numericDF.filter(col(\"Product\") === nextClientProduct))\n",
    "val probabilityVector = nextPredictions.select(\"probability\").head().getAs[org.apache.spark.ml.linalg.DenseVector](0)\n",
    "val positiveClass = probabilityVector(1)\n",
    "val positiveClassFormatted = f\"$positiveClass%.10f\"\n",
    "\n",
    "println(s\"Probabilité de classe positive pour le prochain client qui achète un Iphone : $positiveClassFormatted\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9:Quelle est la probabilité que les prochains clients commandent un téléphone Google ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité de classe positive pour le prochain client qui achète un téléphone Google : 0,0000000494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnextClientProduct\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Google Phone\"\u001b[39m\r\n",
       "\u001b[36mnextPredictions\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 8 more fields]\r\n",
       "\u001b[36mprobabilityVector\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mDenseVector\u001b[39m = [0.9999999505604804,4.943951951903844E-8]\r\n",
       "\u001b[36mpositiveClass\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m4.943951951903844E-8\u001b[39m\r\n",
       "\u001b[36mpositiveClassFormatted\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"0,0000000494\"\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nextClientProduct = \"Google Phone\"\n",
    "\n",
    "val nextPredictions = model.transform(numericDF.filter(col(\"Product\") === nextClientProduct))\n",
    "val probabilityVector = nextPredictions.select(\"probability\").head().getAs[org.apache.spark.ml.linalg.DenseVector](0)\n",
    "val positiveClass = probabilityVector(1)\n",
    "val positiveClassFormatted = f\"$positiveClass%.10f\"\n",
    "\n",
    "println(s\"Probabilité de classe positive pour le prochain client qui achète un téléphone Google : $positiveClassFormatted\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10:Quelle est la probabilité que d'autres personnes commandent des écouteurs filaires ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité de classe positive pour le prochain client qui achète des écouteurs filaires : 0,2132759933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnextClientProduct\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Wired Headphones\"\u001b[39m\r\n",
       "\u001b[36mnextPredictions\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Order ID: string, Product: string ... 8 more fields]\r\n",
       "\u001b[36mprobabilityVector\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mDenseVector\u001b[39m = [0.7867240066586342,0.21327599334136577]\r\n",
       "\u001b[36mpositiveClass\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.21327599334136577\u001b[39m\r\n",
       "\u001b[36mpositiveClassFormatted\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"0,2132759933\"\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nextClientProduct = \"Wired Headphones\"\n",
    "\n",
    "val nextPredictions = model.transform(numericDF.filter(col(\"Product\") === nextClientProduct))\n",
    "val probabilityVector = nextPredictions.select(\"probability\").head().getAs[org.apache.spark.ml.linalg.DenseVector](0)\n",
    "val positiveClass = probabilityVector(1)\n",
    "val positiveClassFormatted = f\"$positiveClass%.10f\"\n",
    "\n",
    "println(s\"Probabilité de classe positive pour le prochain client qui achète des écouteurs filaires : $positiveClassFormatted\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un cas d'utilisation concret de l'apprentissage automatique (Machine Learning) utilisant cet ensemble de données de ventes pourrait être la prévision la demande future de différents produits en fonction des données historiques de ventes ,pour la gestion des stocks, la planification de la production et la prise de décision stratégique en matière de vente\n",
    "\n",
    "\n",
    "#### Modèle prédictif : Un modèle de régression linéaire peut être utilisé pour résoudre ce problème.\n",
    "\n",
    "#### Étapes à suivre :\n",
    "\n",
    "#### Préparation des données : Nettoyez les données en éliminant les valeurs manquantes, en normalisant si nécessaire, et en préparant les caractéristiques pertinentes pour l'entraînement du modèle.\n",
    "\n",
    "#### Séparation des données : Divisez l'ensemble de données en ensembles d'entraînement et de test. Utilisez les données historiques de ventes pour l'entraînement et réservez une partie des données pour évaluer les performances du modèle.\n",
    "\n",
    "#### Création du modèle : Choisissez un modèle approprié pour la prévision de la demande, tel qu'un modèle de régression linéaire. Configurez les hyperparamètres du modèle et entraînez-le sur l'ensemble d'entraînement.\n",
    "\n",
    "#### Évaluation du modèle : Utilisez l'ensemble de test pour évaluer les performances du modèle en comparant les valeurs prédites de la demande avec les valeurs réelles. Utilisez des mesures d'évaluation telles que l'erreur quadratique moyenne (RMSE) ou le coefficient de détermination (R2).\n",
    "\n",
    "#### Prévision de la demande future : Une fois le modèle entraîné et évalué, utilisez-le pour faire des prédictions sur la demande future des produits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
